# @package _global_

# example hyperparameter optimization of some experiment with Optuna:
# python train.py -m hparams_search=mnist_optuna experiment=example



# THIS IS CALLED BY STD_RERUNS.py to allocate seeds and distribute the runs over the slurm cluster.


#@package _global_
defaults:
  - override /hydra/launcher: submitit_slurm
  - override /hydra/sweeper: optuna
  - override /hydra/sweeper/sampler: grid



# choose metric which will be optimized by Optuna
# make sure this is the correct name of some metric logged in lightning module!
# I dont need that for grid?
#optimized_metric: "f1_0.25_val"

# here we define Optuna hyperparameter search
# it optimizes for value returned from function with @hydra.main decorator
# docs: https://hydra.cc/docs/next/plugins/optuna_sweeper
hydra:
  #output_subdir: /home/stein/tester
  launcher: 
    cpus_per_task: 2
    gpus_per_task: null
    exclude: gaia1 gaia4 gaia5 ker gaia3 gaia2
    timeout_min: 6969
    mem_gb: 2
    #nodes: 1
    name: cp_slurm 

  mode: "MULTIRUN" # set hydra to multirun by default if this config is attached


  #sweep:
  #  dir: ${now:%d-%H-%m-%S-%f}

  sweeper:
    _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper

    # storage URL to persist optimization results
    # for example, you can use SQLite if you set 'sqlite:///example.db'
    #storage: null

    # name of the study to persist optimization results
    #study_name: null

    # number of parallel workers
    n_jobs: 600

    # 'minimize' or 'maximize' the objective
    # also dont need that?
    #direction: maximize

    # total number of runs that will be executed
    n_trials: 600
    # choose Optuna hyperparameter sampler
    # you can choose bayesian sampler (tpe), random search (without optimization), grid sampler, and others
    # docs: https://optuna.readthedocs.io/en/stable/reference/samplers.html
    #sampler:

    # _target_: optuna.samplers.GridSampler
      #consider_prior: null
      #seed: 12
      #n_startup_trials: 3 # number of random sampling runs before optimization starts

    # define hyperparameter search space
    # However many you want.
    params:
      #seed: choice(42, 43, 44, 45, 46, 47, 48, 49, 50, 51)
      seed: choice(42, 43, 44, 45, 46)